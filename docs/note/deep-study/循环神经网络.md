# å¾ªç¯ç¥ç»ç½‘ç»œ

## èƒŒæ™¯

å¯¹äºæ™®é€šç¥ç»ç½‘ç»œï¼Œä¾‹å¦‚å¤šå±‚æ„ŸçŸ¥æœºï¼Œå…¶**å‰ä¸€ä¸ªè¾“å…¥å’Œåä¸€ä¸ªè¾“å…¥å®Œå…¨æ²¡æœ‰è”ç³»**ï¼Œå¯¼è‡´åœ¨å¤„ç†æ—¶é—´åºåˆ—é—®é¢˜æ—¶ï¼Œéš¾ä»¥ç²¾å‡†åˆ»ç”»æ—¶é—´åºåˆ—ä¸­çš„æ—¶é—´å…³ç³»ã€‚

ä¸ºäº†æ›´å¥½åœ°å¤„ç†æ—¶é—´åºåˆ—é—®é¢˜ï¼Œå­¦è€…æå‡ºäº†å¾ªç¯ç¥ç»ç½‘ç»œç»“æ„ï¼ˆRecurrent Neural Networkï¼‰ï¼Œæœ€åŸºæœ¬çš„å¾ªç¯ç¥ç»ç½‘ç»œç”±è¾“å…¥å±‚ã€ä¸€ä¸ªéšè—å±‚å’Œä¸€ä¸ªè¾“å‡ºå±‚ç»„æˆã€‚

## RNN ç®€ä»‹

å¾ªç¯ç¥ç»ç½‘ç»œç»“æ„ï¼ˆRecurrent Neural Networkï¼‰

![image-20250510094123603](markdown-img/å¾ªç¯ç¥ç»ç½‘ç»œ.assets/image-20250510094123603.png)

è§£é‡Šï¼š

- Xï¼šè¾“å…¥å±‚çš„å€¼
- Uï¼šè¾“å…¥åˆ°éšè—å±‚çš„æƒé‡çŸ©é˜µ
- Vï¼šéšè—å±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡çŸ©é˜µ
- Oï¼šè¾“å‡ºå±‚
- Sï¼šæ˜¯ä¸€ä¸ªå‘é‡ï¼Œè¡¨ç¤ºéšè—å±‚çš„å€¼ï¼Œå…¶ä¸ä»…ä»…å–å†³äºå½“å‰çš„è¾“å…¥Xï¼Œ**è¿˜å–å†³äºä¸Šä¸€æ—¶åˆ»éšè—å±‚çš„å€¼**ã€‚å¦‚æœå»æ‰æœ‰Wçš„å¸¦ç®­å¤´çš„è¿æ¥çº¿ï¼Œå³ä¸ºæ™®é€šçš„å…¨è¿æ¥ç¥ç»ç½‘ç»œã€‚
- Wï¼šéšè—å±‚ä¸Šä¸€æ—¶åˆ»çš„å€¼ï¼Œä½œä¸ºå½“å‰æ—¶åˆ»è¾“å…¥çš„æƒé‡çŸ©é˜µ

å…¬å¼ï¼š

![image-20250510094337867](markdown-img/å¾ªç¯ç¥ç»ç½‘ç»œ.assets/image-20250510094337867.png)

è§£é‡Šï¼š

+ få³ä¸ºæ¿€æ´»å‡½æ•°

æŠŠRNNç»“æ„æŒ‰ç…§æ—¶é—´çº¿å±•å¼€ã€‚ï¼ˆElmanï¼‰

![image-20250510094424957](markdown-img/å¾ªç¯ç¥ç»ç½‘ç»œ.assets/image-20250510094424957.png)

è‹¥åªè¦ä¸€ä¸ªè¾“å‡ºï¼Œä¸è¦Otã€Ot-1å³å¯ã€‚

![image-20250510094903687](markdown-img/å¾ªç¯ç¥ç»ç½‘ç»œ.assets/image-20250510094903687.png)

RNNä¸åŸºç¡€çš„ç¥ç»ç½‘ç»œæœ€å¤§çš„ä¸åŒä¹‹å¤„åœ¨äº**åŒä¸€å±‚å†…çš„ç¥ç»å…ƒåœ¨ä¸åŒæ—¶åˆ»ä¹Ÿå»ºç«‹äº†å…¨è¿æ¥**ï¼Œå³Wä¸æ—¶é—´æœ‰å…³ã€‚

## LSTM

LSTMï¼ˆLong Short-Term Memoryï¼‰æ¨¡å‹ï¼Œå±äºRNNçš„ä¸€ç§ã€‚

å¾ªç¯ç¥ç»ç½‘ç»œå›¾LSTM

![image-20250510100727781](markdown-img/å¾ªç¯ç¥ç»ç½‘ç»œ.assets/image-20250510100727781.png)

![image-20250510095939720](markdown-img/å¾ªç¯ç¥ç»ç½‘ç»œ.assets/image-20250510095939720.png)

### ä¸RNNåŒºåˆ«

RNNå’ŒLSTMæœ€å¤§åŒºåˆ«åœ¨äºåˆ†å¸ƒåœ¨éšè—å±‚çš„ç¥ç»å…ƒç»“æ„ã€‚

LSTMçš„è®°å¿†å•å…ƒï¼ˆBlockï¼‰æ›´åŠ å¤æ‚ï¼Œ**LSTMæ¨¡å‹ä¸­å¢åŠ äº†çŠ¶æ€ğ‘**ï¼Œç§°ä¸ºå•å…ƒçŠ¶æ€ï¼ˆcell stateï¼‰ï¼Œç”¨æ¥**ä¿å­˜é•¿æœŸçš„çŠ¶æ€**ï¼Œè€ŒLSTMçš„å…³é”®ï¼Œå°±æ˜¯æ€æ ·æ§åˆ¶é•¿æœŸçŠ¶æ€

### LSTMçš„é‡å¤æ¨¡å—

çœŸå®å›¾ï¼š

![image-20250510100021713](markdown-img/å¾ªç¯ç¥ç»ç½‘ç»œ.assets/image-20250510100021713.png)

å…¬å¼ï¼š

![image-20250510100052578](markdown-img/å¾ªç¯ç¥ç»ç½‘ç»œ.assets/image-20250510100052578.png)

LSTMçš„æ ¸å¿ƒæ˜¯å•å…ƒçš„çŠ¶æ€ï¼Œå•å…ƒçŠ¶æ€çš„ä¼ é€’ç±»ä¼¼äºä¼ é€å¸¦ï¼Œç›´æ¥åœ¨æ•´ä¸ªæ—¶é—´é“¾ä¸Šè¿è¡Œï¼Œä¸­é—´å€¼æœ‰ä¸€äº›å°‘é‡çš„çº¿æ€§äº¤äº’ï¼Œä¾¿äºä¿å­˜ç›¸å…³ä¿¡æ¯ã€‚

### å®ç°

1. å¯¼åŒ…


```python
from torch import nn
from torch.autograd import Variable
import torch

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
```

2. æ•°æ®å±•ç¤º


```python
file_path = './LSTM.csv'
data = pd.read_csv(file_path)
data.dropna(inplace=True)
plt.plot(data)
plt.show()
```


â€‹    
![png](markdown-img/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.assets/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_4_0.png)
â€‹    


3. æ•°æ®å½’ä¸€åŒ–

![image.png](markdown-img/å¾ªç¯ç¥ç»ç½‘ç»œ.assets/3120cdea-49b2-4ba7-bb52-e51971f67678.png)


```python
# å½’ä¸€åŒ– æ‰€æœ‰æ•°æ®ç¼©æ”¾åˆ° 0å’Œ1ä¹‹é—´
data_ndarray = data.values
data_ndarray = data_ndarray.astype('float32')
max_num = np.max(data_ndarray)
min_num = np.min(data_ndarray)
scalar = max_num - min_num

dataset = list(map(lambda x: (x - min_num)/scalar,data_ndarray))
```

4. åˆå§‹åŒ–è®­ç»ƒå’Œæµ‹è¯•æ•°æ®


```python
'''
åˆå§‹åŒ–æ•°æ®
æŠŠå‰2ä¸ªæ—¶é—´ç²’åº¦çš„å®¢æµæ•°æ®ä½œä¸ºè¾“å…¥,æŠŠå½“å‰æ—¶é—´ç²’åº¦çš„å®¢æµæ•°æ®ä½œä¸ºè¾“å‡º
è¿”å›ï¼šæµ‹è¯•é›†ã€è®­ç»ƒé›†
'''
def create_dataset(data,step=5, train_percent=0.7):
    dataX = []
    dataY = []
    data_len = len(data)
    for i in np.arange(data_len - step):
        split_num = i+step
        dataX.append(data[i:split_num])
        dataY.append(data[split_num])

    # åŒºåˆ†æµ‹è¯•ã€è®­ç»ƒé›†
    dataX_len = len(dataX)
    dataY_len = len(dataY)
    split_index = int(dataX_len * train_percent)
    
    trainX = torch.from_numpy(np.array(dataX[:split_index]).reshape(-1,1,5))
    trainY = torch.from_numpy(np.array(dataY[:split_index]).reshape(-1,1,1))
    testX = torch.from_numpy(np.array(dataX[split_index:]).reshape(-1,1,5))
    testY = torch.from_numpy(np.array(dataY[split_index:]).reshape(-1,1,1))

    return trainX, trainY, testX, testY

trainX, trainY, testX, testY = create_dataset(dataset)
```

5. å»ºç«‹æ¨¡å‹

   åœ¨ `nn.Module` å†…éƒ¨ï¼Œæœ‰ **`__call__()` é­”æ³•æ–¹æ³•**ã€‚

   å½“ä½ â€œè°ƒç”¨â€ä¸€ä¸ªå¯¹è±¡æ—¶ï¼ŒPython ä¼šè‡ªåŠ¨è§¦å‘è¿™ä¸ªå¯¹è±¡çš„ __call__ æ–¹æ³•ï¼šåœ¨`__init__`ä¹‹åï¼Œä¼šè‡ªåŠ¨æ‰§è¡Œ`__call__`

   ```
   def __call__(self, *input, **kwargs):
       return self.forward(*input, **kwargs)
   ```


```python
class LSTM_LINEAR(nn.Module):
    def __init__(self, input_size, hidden_size, output_size=1, num_layers=2):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)
        self.linear = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x, _ = self.lstm(x) # (seq, batch, hidden)
        s,b,h = x.shape
        # é™ç»´ï¼ŒåŒ¹é…linearå‚æ•°è¦æ±‚
        x = x.view(s*b, h)
        x = self.linear(x) # input:(*, Hin), Hinå³ hidden_size
        x = x.view(s,b,-1)
        return x

# å®ä¾‹åŒ–
net = LSTM_LINEAR(5,45)
# æŸå¤±
criterion = nn.MSELoss()
# ä¼˜åŒ–å™¨
optimizer = torch.optim.Adam(net.parameters(), lr=1e-2)
```

6. è®­ç»ƒ


```python
# è®­ç»ƒ
for e in range(200):
    out = net(trainX)
    loss = criterion(out, trainY)
    # æ¢¯åº¦æ¸…é›¶
    optimizer.zero_grad()
    loss.backward()
    # æ›´æ–°æ¨¡å‹å‚æ•°
    optimizer.step()
    if e%10==0:
        print(f'epoch: {e}, loss: {loss.item()}')
```

    epoch: 0, loss: 0.23863691091537476
    epoch: 10, loss: 0.04517023265361786
    epoch: 20, loss: 0.04236963018774986
    epoch: 30, loss: 0.03165356442332268
    epoch: 40, loss: 0.02464795857667923
    epoch: 50, loss: 0.017756391316652298
    epoch: 60, loss: 0.010942698456346989
    epoch: 70, loss: 0.0064301323145627975
    epoch: 80, loss: 0.004891194868832827
    epoch: 90, loss: 0.004182700999081135
    epoch: 100, loss: 0.0035117552615702152
    epoch: 110, loss: 0.0030126941855996847
    epoch: 120, loss: 0.002448414918035269
    epoch: 130, loss: 0.0033389576710760593
    epoch: 140, loss: 0.0027324846014380455
    epoch: 150, loss: 0.0020164402667433023
    epoch: 160, loss: 0.0015253744786605239
    epoch: 170, loss: 0.001150979078374803
    epoch: 180, loss: 0.0025168675929307938
    epoch: 190, loss: 0.001205406035296619


7. æµ‹è¯•


```python
# è½¬ä¸ºæµ‹è¯•æ¨¡å¼
net = net.eval()
with torch.no_grad():
    out = net(testX)
    loss = criterion(out, testY)

print(f'sample is test, loss is {loss.item()}')
```

    sample is test, loss is 0.001995484111830592


8. é¢„æµ‹å¹¶åå½’ä¸€åŒ–


```python
# åå½’ä¸€åŒ–
def restore(x):
    return x.view(-1).data.numpy()*scalar +min_num

pre_testX = net(testX)
pre_testX_new = restore(pre_testX) # é¢„æµ‹å€¼
testY_new = restore(testY) # çœŸå®å€¼
```

9. ç”»å›¾

```python
plt.figure(figsize=(6,4))
plt.plot(pre_testX_new, label='predict')
plt.plot(testY_new, label='real')

plt.show()
```
