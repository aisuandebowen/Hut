(window.webpackJsonp=window.webpackJsonp||[]).push([[62],{473:function(a,t,s){"use strict";s.r(t);var e=s(2),r=Object(e.a)({},(function(){var a=this,t=a._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[t("h1",{attrs:{id:"优化器模块"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#优化器模块"}},[a._v("#")]),a._v(" 优化器模块")]),a._v(" "),t("p",[a._v("优化参数的具体机制。工具包："),t("code",[a._v("torch.optim")])]),a._v(" "),t("h2",{attrs:{id:"使用optimizer"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#使用optimizer"}},[a._v("#")]),a._v(" 使用optimizer")]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("optimizer = optim.Adam(net.parameters(), lr =0.001)\n# 单独设置参数 \noptimizer = optim.Adam([{'params':model.base.parameters()},{'params':model.regression.parameters(), 'lr': 0.0001}], lr = 0.001)\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br"),t("span",{staticClass:"line-number"},[a._v("2")]),t("br"),t("span",{staticClass:"line-number"},[a._v("3")]),t("br")])]),t("p",[a._v("其中，")]),a._v(" "),t("ul",[t("li",[a._v("net.parameters：所有参数")]),a._v(" "),t("li",[a._v("lr：学习率")])]),a._v(" "),t("h2",{attrs:{id:"常见优化器"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#常见优化器"}},[a._v("#")]),a._v(" 常见优化器")]),a._v(" "),t("ul",[t("li",[a._v("梯度下降")]),a._v(" "),t("li",[a._v("逐参数适应学习率方法")])]),a._v(" "),t("h3",{attrs:{id:"梯度下降法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#梯度下降法"}},[a._v("#")]),a._v(" 梯度下降法")]),a._v(" "),t("h4",{attrs:{id:"批量梯度下降-batch-gradient-descent"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#批量梯度下降-batch-gradient-descent"}},[a._v("#")]),a._v(" 批量梯度下降（batch gradient descent）")]),a._v(" "),t("p",[a._v("通过对"),t("strong",[a._v("所有样本")]),a._v("的计算来求解梯度的方向，梯度方差小。")]),a._v(" "),t("p",[t("img",{attrs:{src:"markdown-img/%E4%BC%98%E5%8C%96%E5%99%A8%E6%A8%A1%E5%9D%97.assets/image-20250507154733637.png",alt:"image-20250507154733637"}})]),a._v(" "),t("p",[a._v("缺点：需要较多的计算资源")]),a._v(" "),t("h4",{attrs:{id:"随机梯度下降-stochastic-gradient-descent"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#随机梯度下降-stochastic-gradient-descent"}},[a._v("#")]),a._v(" 随机梯度下降（stochastic gradient descent）")]),a._v(" "),t("p",[a._v("每次"),t("strong",[a._v("随机选取一个样本的损失函数来求梯度")]),a._v("，训练速度快。")]),a._v(" "),t("p",[t("img",{attrs:{src:"markdown-img/%E4%BC%98%E5%8C%96%E5%99%A8%E6%A8%A1%E5%9D%97.assets/image-20250507154748556.png",alt:"image-20250507154748556"}})]),a._v(" "),t("p",[a._v("缺点：")]),a._v(" "),t("ol",[t("li",[a._v("方差大，损失震荡严重。")]),a._v(" "),t("li",[a._v("由于鞍点的存在可能导致局部梯度为零，无法继续移动，使得最优解可能仅为局部最优。")])]),a._v(" "),t("h4",{attrs:{id:"小批量梯度下降-mini-batch-gradient-descent"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#小批量梯度下降-mini-batch-gradient-descent"}},[a._v("#")]),a._v(" 小批量梯度下降（mini-batch gradient descent）")]),a._v(" "),t("p",[a._v("即"),t("strong",[a._v("把数据分为若干个批，按批量来更新参数")]),a._v("。")]),a._v(" "),t("p",[t("img",{attrs:{src:"markdown-img/%E4%BC%98%E5%8C%96%E5%99%A8%E6%A8%A1%E5%9D%97.assets/image-20250507154910321.png",alt:"image-20250507154910321"}})]),a._v(" "),t("p",[a._v("优点：减少了梯度下降的随机性，也减少了计算量。")]),a._v(" "),t("p",[a._v("缺点：该方法选择一个合适的学习率比较困难；且梯度容易被困在鞍点。")]),a._v(" "),t("h4",{attrs:{id:"动量优化算法-momentum"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#动量优化算法-momentum"}},[a._v("#")]),a._v(" 动量优化算法（Momentum）")]),a._v(" "),t("p",[a._v("通过使用最近一段时间内的平均梯度来代替当前时刻的随机梯度作为参数更新的方向，从而提高优化速度。")]),a._v(" "),t("p",[t("img",{attrs:{src:"markdown-img/%E4%BC%98%E5%8C%96%E5%99%A8%E6%A8%A1%E5%9D%97.assets/image-20250507154954963.png",alt:"image-20250507154954963"}})]),a._v(" "),t("p",[a._v("即"),t("strong",[a._v("过去方向+当前方向的加权平均和")]),a._v("（"),t("strong",[a._v("惯性")]),a._v("）")]),a._v(" "),t("h3",{attrs:{id:"逐参数适应学习率方法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#逐参数适应学习率方法"}},[a._v("#")]),a._v(" 逐参数适应学习率方法")]),a._v(" "),t("h4",{attrs:{id:"adagrad"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#adagrad"}},[a._v("#")]),a._v(" AdaGrad")]),a._v(" "),t("p",[a._v("基本思想是对"),t("strong",[a._v("每个变量采用不同的学习率")]),a._v("。")]),a._v(" "),t("p",[a._v("学习率在一开始比较大，用于快速梯度下降；随着优化过程进行，对于已经下降很多的变量，减缓学习率；对于还没怎么下降的变量，则保持较大的学习率。")]),a._v(" "),t("p",[a._v("更新参数公式：")]),a._v(" "),t("p",[t("img",{attrs:{src:"markdown-img/%E4%BC%98%E5%8C%96%E5%99%A8%E6%A8%A1%E5%9D%97.assets/image-20250507155453899.png",alt:"image-20250507155453899"}})]),a._v(" "),t("p",[a._v("∑gt^2即为梯度的平方累加。")]),a._v(" "),t("p",[a._v("代码：")]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("optimizer = torch.optim.Adagrad(params, lr=0.01, lr_decay=0,weight_decay=0, initial_accumulator_value=0)\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br")])]),t("p",[a._v("缺点：当下降太快，后续调参不好调。")]),a._v(" "),t("h4",{attrs:{id:"rmsprop"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#rmsprop"}},[a._v("#")]),a._v(" RMSProp")]),a._v(" "),t("p",[a._v("不像AdaGrad算法那样直接累加平方梯度，而是加了一个"),t("strong",[a._v("衰减系数")]),a._v("来控制历史信息的获取多少，即做了一个梯度平方的滑动平均。（类似momentum惯性）")]),a._v(" "),t("p",[t("img",{attrs:{src:"markdown-img/%E4%BC%98%E5%8C%96%E5%99%A8%E6%A8%A1%E5%9D%97.assets/image-20250507155836708.png",alt:"image-20250507155836708"}})]),a._v(" "),t("p",[a._v("代码：")]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("optimizer = torch.optim.RMSprop(params, lr=0.01, alpha=0.99,eps=1e-08,weight_decay=0,momentum=0,centered=False)\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br")])]),t("h4",{attrs:{id:"adam"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#adam"}},[a._v("#")]),a._v(" Adam")]),a._v(" "),t("p",[a._v("即自适应时刻估计方法（Adaptive Moment Estimation）相当于自适应学习率（RMSProp）和动量法（Momentum ）的相结合，能够计算每个参数的自适应学习率，将"),t("strong",[a._v("惯性保持和环境感知这两个优点集于一身")]),a._v("。")]),a._v(" "),t("p",[a._v("代码：")]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("optimizer = torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999),eps=1e-08, weight_decay=0)\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br")])])])}),[],!1,null,null,null);t.default=r.exports}}]);