(window.webpackJsonp=window.webpackJsonp||[]).push([[73],{482:function(t,s,a){"use strict";a.r(s);var n=a(2),r=Object(n.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"_4-激活函数"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-激活函数"}},[t._v("#")]),t._v(" 4 激活函数")]),t._v(" "),s("p",[t._v("torch.nn.functional")]),t._v(" "),s("h2",{attrs:{id:"_4-1-简介"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-1-简介"}},[t._v("#")]),t._v(" 4.1 简介")]),t._v(" "),s("p",[t._v("在多层神经网络中，上层节点的输出和下层节点的输入之间有一个函数关系。如果这个函数我们设置为非线性函数，深层网络的表达能力将会大幅度提升，几乎可以逼近任何函数，我们把这些非线性函数叫做激活函数。")]),t._v(" "),s("p",[t._v("用途：激活函数的作用就是"),s("strong",[t._v("给网络提供非线性的建模能力")]),t._v("。")]),t._v(" "),s("h2",{attrs:{id:"_4-2-常用激活函数"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-常用激活函数"}},[t._v("#")]),t._v(" 4.2 常用激活函数")]),t._v(" "),s("h3",{attrs:{id:"sigmoid函数"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#sigmoid函数"}},[t._v("#")]),t._v(" Sigmoid函数")]),t._v(" "),s("p",[t._v("torch.sigmoid()")]),t._v(" "),s("p",[s("img",{attrs:{src:"markdown-img/4%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.assets/6c4df72f-cec4-40b6-9dab-df6f1f22eadd.png",alt:"image.png"}})]),t._v(" "),s("p",[t._v("优点：很好地解释了神经元在受到刺激的情况下是否被激活和向后传递的情景。当取值接近0时几乎没有被激活，当取值接近1的时候几乎完全被激活。")]),t._v(" "),s("p",[t._v("缺点：")]),t._v(" "),s("ul",[s("li",[t._v("容易出现梯度消失(0.9^100)，甚至小概率会出现梯度爆炸(1.1^100)问题")]),t._v(" "),s("li",[t._v("含有幂函数，计算机在求解的时候比较耗时")]),t._v(" "),s("li",[t._v("输出不是0均值。（经验而言，均值0效果更好）")])]),t._v(" "),s("div",{staticClass:"language-python line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch\n\nx "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rand"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\noutput "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sigmoid"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br"),s("span",{staticClass:"line-number"},[t._v("5")]),s("br"),s("span",{staticClass:"line-number"},[t._v("6")]),s("br")])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("tensor([0.6491, 0.5927, 0.7311, 0.6114])\n")])])]),s("h3",{attrs:{id:"tanh函数"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#tanh函数"}},[t._v("#")]),t._v(" Tanh函数")]),t._v(" "),s("p",[t._v("本质是sigmoid函数的一个"),s("strong",[t._v("变形")]),t._v(",两者的关系为"),s("code",[t._v("tanh(x)=2sigmoid(2x)-1")]),t._v(" "),s("img",{attrs:{src:"markdown-img/4%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.assets/057e6004-1052-4e20-9625-5d527559d8e4.png",alt:"image.png"}})]),t._v(" "),s("p",[t._v("优势：")]),t._v(" "),s("ul",[s("li",[t._v("将输出值映射到（-1,1）之间，因此解决了sigmoid函数的非0均值问题")])]),t._v(" "),s("p",[t._v("缺点：")]),t._v(" "),s("ul",[s("li",[t._v("存在梯度消失和梯度爆炸的问题")]),t._v(" "),s("li",[t._v("幂运算也会导致计算耗时久")])]),t._v(" "),s("p",[t._v("注意：为了防止饱和情况的发生，在激活函数前可以加一步batch normalization，尽可能的保证神经网络的输入在每一层都具有均值较小的0中心分布")]),t._v(" "),s("h3",{attrs:{id:"relu函数-线性"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#relu函数-线性"}},[t._v("#")]),t._v(" Relu函数(线性)")]),t._v(" "),s("p",[t._v("Relu是修正线性单元（The Rectified Linear Unit）的简称\n"),s("img",{attrs:{src:"markdown-img/4%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.assets/fe762381-0eaf-42d5-836d-c40f0b59ceb5.png",alt:"image.png"}})]),t._v(" "),s("p",[t._v("优势：")]),t._v(" "),s("ul",[s("li",[t._v("不存在指数运算部分，几乎没有什么计算量")]),t._v(" "),s("li",[t._v("具有单侧抑制(负无穷,0)、宽兴奋(0,正无穷)边界的生物学合理性")])]),t._v(" "),s("p",[t._v("缺点：有时由于输出反复为0，神经元可能会死亡。")]),t._v(" "),s("h3",{attrs:{id:"leakyrelu函数"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#leakyrelu函数"}},[t._v("#")]),t._v(" LeakyRelu函数")]),t._v(" "),s("p",[t._v("解决一部分Relu函数存在的可能杀死神经元的问题（输出加了个斜率）")]),t._v(" "),s("p",[s("img",{attrs:{src:"markdown-img/4%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.assets/1ce235d0-3ec7-415d-80a5-c6c61e957267.png",alt:"image.png"}})]),t._v(" "),s("div",{staticClass:"language-python line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch\nx "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rand"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ny "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("functional"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("leaky_relu"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.03")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br")])]),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("tensor([0.4088, 0.5671, 0.3428, 0.2739, 0.9709, 0.2129, 0.8612, 0.4952, 0.5532,\n        0.0693])\n")])])]),s("h2",{attrs:{id:"_4-3-nn模块其他常用方法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-3-nn模块其他常用方法"}},[t._v("#")]),t._v(" 4.3 nn模块其他常用方法")]),t._v(" "),s("p",[t._v("torch.nn模块，即“Neural Networks（神经网络）”模块，是提供构建神经网络的各种“组件”与“工具”。")]),t._v(" "),s("p",[t._v("这些“组件”包括：")]),t._v(" "),s("ul",[s("li",[t._v("神经网络层（linear、conv、lstm…）")]),t._v(" "),s("li",[t._v("激活函数（ReLU、Sigmoid、Tanh…）")]),t._v(" "),s("li",[t._v("损失函数（MSELoss、CrossEntropyLoss…）")]),t._v(" "),s("li",[t._v("正则化层（BatchNorm、Dropout…）")]),t._v(" "),s("li",[t._v("容器模块（Sequential、ModuleList、ModuleDict…）")])]),t._v(" "),s("h3",{attrs:{id:"nn-linear"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#nn-linear"}},[t._v("#")]),t._v(" nn.Linear")]),t._v(" "),s("p",[t._v("nn.Linear(in_features, out_features, bias=True)")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("参数")]),t._v(" "),s("th",[t._v("含义")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[s("code",[t._v("in_features")])]),t._v(" "),s("td",[t._v("输入特征的维度")])]),t._v(" "),s("tr",[s("td",[s("code",[t._v("out_features")])]),t._v(" "),s("td",[t._v("输出特征的维度")])]),t._v(" "),s("tr",[s("td",[s("code",[t._v("bias")])]),t._v(" "),s("td",[t._v("是否加偏置项（默认 "),s("code",[t._v("True")]),t._v("）")])])])]),t._v(" "),s("p",[t._v("nn.Linear 实现了一个 线性变换（矩阵乘+偏置），输入一个 shape 为 (batch_size, in_features) 的张量，输出 shape 为 (batch_size, out_features)，是深度学习中最常用的“全连接层”。")]),t._v(" "),s("p",[t._v("常用于：")]),t._v(" "),s("ul",[s("li",[t._v("输入 → 隐藏层映射")]),t._v(" "),s("li",[t._v("隐藏层 → 输出层映射")]),t._v(" "),s("li",[t._v("LSTM / CNN 的输出 → 分类层")])]),t._v(" "),s("h3",{attrs:{id:"nn-mseloss"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#nn-mseloss"}},[t._v("#")]),t._v(" nn.MSELoss")]),t._v(" "),s("p",[t._v("均方误差损失函数")]),t._v(" "),s("p",[s("img",{attrs:{src:"markdown-img/4%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.assets/MSELoss.png",alt:"image.png"}})]),t._v(" "),s("p",[t._v("常用")]),t._v(" "),s("div",{staticClass:"language- line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("loss = nn.MSELoss()\ninput = torch.randn(3, 5, requires_grad=True)\ntarget = torch.randn(3, 5)\noutput = loss(input, target)\noutput.backward()\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br"),s("span",{staticClass:"line-number"},[t._v("5")]),s("br")])]),s("h3",{attrs:{id:"nn-crossentropyloss"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#nn-crossentropyloss"}},[t._v("#")]),t._v(" nn.CrossEntropyLoss")]),t._v(" "),s("p",[t._v("交叉熵损失函数")]),t._v(" "),s("h3",{attrs:{id:"常见-loss"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#常见-loss"}},[t._v("#")]),t._v(" 常见 loss：")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("Loss 类型")]),t._v(" "),s("th",[t._v("场景")]),t._v(" "),s("th",[t._v("为啥不能简单做“预测 - 真实”")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[s("strong",[t._v("MSE（均方误差）")])]),t._v(" "),s("td",[t._v("回归")]),t._v(" "),s("td",[t._v("适合连续数值，差的平方反映偏差")])]),t._v(" "),s("tr",[s("td",[s("strong",[t._v("BCE（二元交叉熵）")])]),t._v(" "),s("td",[t._v("二分类（概率输出）")]),t._v(" "),s("td",[s("code",[t._v("预测 - 标签")]),t._v(" 不是概率上的“差距”")])]),t._v(" "),s("tr",[s("td",[s("strong",[t._v("Cross Entropy")])]),t._v(" "),s("td",[t._v("多分类")]),t._v(" "),s("td",[t._v("预测是概率分布，标签是分类索引，“误差”必须反映整个分布的差异")])]),t._v(" "),s("tr",[s("td",[s("strong",[t._v("KL 散度")])]),t._v(" "),s("td",[t._v("分布对分布")]),t._v(" "),s("td",[t._v("衡量两个分布之间的信息损失，“差值”根本说不清")])]),t._v(" "),s("tr",[s("td",[s("strong",[t._v("Hinge Loss")])]),t._v(" "),s("td",[t._v("SVM")]),t._v(" "),s("td",[t._v("要求 margin，而不是概率/值差距")])])])]),t._v(" "),s("h3",{attrs:{id:"qr"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#qr"}},[t._v("#")]),t._v(" QR")]),t._v(" "),s("ol",[s("li",[s("p",[t._v("为什么学习规则有那么多种？不就是计算loss 么， 不直接真实和预测之差就行了")]),t._v(" "),s("h2",{attrs:{id:"🔍-类比解释"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#🔍-类比解释"}},[t._v("#")]),t._v(" 🔍 类比解释：")]),t._v(" "),s("p",[t._v("想象你在不同场景下扔飞镖：")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("场景")]),t._v(" "),s("th",[t._v("目标形状")]),t._v(" "),s("th",[t._v("应该怎么评价你扔得准不准？")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("标准靶子")]),t._v(" "),s("td",[t._v("一个点")]),t._v(" "),s("td",[t._v("距离中心的距离（平方差）")])]),t._v(" "),s("tr",[s("td",[t._v("猜硬币正反")]),t._v(" "),s("td",[t._v("二选一")]),t._v(" "),s("td",[t._v("猜中 vs 猜错（二元交叉熵）")])]),t._v(" "),s("tr",[s("td",[t._v("猜骰子点数")]),t._v(" "),s("td",[t._v("六选一")]),t._v(" "),s("td",[t._v("哪面概率最高（交叉熵）")])]),t._v(" "),s("tr",[s("td",[t._v("猜多个标签")]),t._v(" "),s("td",[t._v("可多选")]),t._v(" "),s("td",[t._v("每个标签独立判断（多标签 BCE）")])])])]),t._v(" "),s("p",[t._v("你不会用“扔远了几米”去评价一个猜正反面的问题，对吧？同理，"),s("strong",[t._v("不同类型的问题 → 不同的误差评价方式（损失函数）")]),t._v("。")])])])])}),[],!1,null,null,null);s.default=r.exports}}]);