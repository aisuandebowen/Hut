(window.webpackJsonp=window.webpackJsonp||[]).push([[73],{484:function(s,t,a){"use strict";a.r(t);var n=a(2),r=Object(n.a)({},(function(){var s=this,t=s._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[t("h1",{attrs:{id:"_4-激活函数"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-激活函数"}},[s._v("#")]),s._v(" 4 激活函数")]),s._v(" "),t("p",[s._v("torch.nn.functional")]),s._v(" "),t("h2",{attrs:{id:"_4-1-简介"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-1-简介"}},[s._v("#")]),s._v(" 4.1 简介")]),s._v(" "),t("p",[s._v("在多层神经网络中，上层节点的输出和下层节点的输入之间有一个函数关系。如果这个函数我们设置为非线性函数，深层网络的表达能力将会大幅度提升，几乎可以逼近任何函数，我们把这些非线性函数叫做激活函数。")]),s._v(" "),t("p",[s._v("用途：激活函数的作用就是"),t("strong",[s._v("给网络提供非线性的建模能力")]),s._v("。")]),s._v(" "),t("h2",{attrs:{id:"_4-2-常用激活函数"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-常用激活函数"}},[s._v("#")]),s._v(" 4.2 常用激活函数")]),s._v(" "),t("h3",{attrs:{id:"sigmoid函数"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#sigmoid函数"}},[s._v("#")]),s._v(" Sigmoid函数")]),s._v(" "),t("p",[s._v("torch.sigmoid()")]),s._v(" "),t("p",[t("img",{attrs:{src:"markdown-img/4%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.assets/6c4df72f-cec4-40b6-9dab-df6f1f22eadd.png",alt:"image.png"}})]),s._v(" "),t("p",[s._v("优点：很好地解释了神经元在受到刺激的情况下是否被激活和向后传递的情景。当取值接近0时几乎没有被激活，当取值接近1的时候几乎完全被激活。")]),s._v(" "),t("p",[s._v("缺点：")]),s._v(" "),t("ul",[t("li",[s._v("容易出现梯度消失(0.9^100)，甚至小概率会出现梯度爆炸(1.1^100)问题")]),s._v(" "),t("li",[s._v("含有幂函数，计算机在求解的时候比较耗时")]),s._v(" "),t("li",[s._v("输出不是0均值。（经验而言，均值0效果更好）")])]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" torch\n\nx "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" torch"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("rand"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\noutput "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" torch"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("sigmoid"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("x"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("output"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br")])]),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[s._v("tensor([0.6491, 0.5927, 0.7311, 0.6114])\n")])])]),t("h3",{attrs:{id:"tanh函数"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#tanh函数"}},[s._v("#")]),s._v(" Tanh函数")]),s._v(" "),t("p",[s._v("本质是sigmoid函数的一个"),t("strong",[s._v("变形")]),s._v(",两者的关系为"),t("code",[s._v("tanh(x)=2sigmoid(2x)-1")]),s._v(" "),t("img",{attrs:{src:"markdown-img/4%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.assets/057e6004-1052-4e20-9625-5d527559d8e4.png",alt:"image.png"}})]),s._v(" "),t("p",[s._v("优势：")]),s._v(" "),t("ul",[t("li",[s._v("将输出值映射到（-1,1）之间，因此解决了sigmoid函数的非0均值问题")])]),s._v(" "),t("p",[s._v("缺点：")]),s._v(" "),t("ul",[t("li",[s._v("存在梯度消失和梯度爆炸的问题")]),s._v(" "),t("li",[s._v("幂运算也会导致计算耗时久")])]),s._v(" "),t("p",[s._v("注意：为了防止饱和情况的发生，在激活函数前可以加一步batch normalization，尽可能的保证神经网络的输入在每一层都具有均值较小的0中心分布")]),s._v(" "),t("h3",{attrs:{id:"relu函数-线性"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#relu函数-线性"}},[s._v("#")]),s._v(" Relu函数(线性)")]),s._v(" "),t("p",[s._v("Relu是修正线性单元（The Rectified Linear Unit）的简称\n"),t("img",{attrs:{src:"markdown-img/4%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.assets/fe762381-0eaf-42d5-836d-c40f0b59ceb5.png",alt:"image.png"}})]),s._v(" "),t("p",[s._v("优势：")]),s._v(" "),t("ul",[t("li",[s._v("不存在指数运算部分，几乎没有什么计算量")]),s._v(" "),t("li",[s._v("具有单侧抑制(负无穷,0)、宽兴奋(0,正无穷)边界的生物学合理性")])]),s._v(" "),t("p",[s._v("缺点：有时由于输出反复为0，神经元可能会死亡。")]),s._v(" "),t("h3",{attrs:{id:"leakyrelu函数"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#leakyrelu函数"}},[s._v("#")]),s._v(" LeakyRelu函数")]),s._v(" "),t("p",[s._v("解决一部分Relu函数存在的可能杀死神经元的问题（输出加了个斜率）")]),s._v(" "),t("p",[t("img",{attrs:{src:"markdown-img/4%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.assets/1ce235d0-3ec7-415d-80a5-c6c61e957267.png",alt:"image.png"}})]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" torch\nx "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" torch"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("rand"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("10")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\ny "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" torch"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("nn"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("functional"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("leaky_relu"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("x"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.03")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("y"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br")])]),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[s._v("tensor([0.4088, 0.5671, 0.3428, 0.2739, 0.9709, 0.2129, 0.8612, 0.4952, 0.5532,\n        0.0693])\n")])])]),t("h2",{attrs:{id:"nn模块其他常用方法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#nn模块其他常用方法"}},[s._v("#")]),s._v(" nn模块其他常用方法")]),s._v(" "),t("p",[s._v("torch.nn模块，即“Neural Networks（神经网络）”模块，是提供构建神经网络的各种“组件”与“工具”。")]),s._v(" "),t("p",[s._v("这些“组件”包括：")]),s._v(" "),t("ul",[t("li",[s._v("神经网络层（linear、conv、lstm…）")]),s._v(" "),t("li",[s._v("激活函数（ReLU、Sigmoid、Tanh…）")]),s._v(" "),t("li",[s._v("损失函数（MSELoss、CrossEntropyLoss…）")]),s._v(" "),t("li",[s._v("正则化层（BatchNorm、Dropout…）")]),s._v(" "),t("li",[s._v("容器模块（Sequential、ModuleList、ModuleDict…）")])]),s._v(" "),t("h3",{attrs:{id:"nn-linear"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#nn-linear"}},[s._v("#")]),s._v(" nn.Linear")]),s._v(" "),t("p",[s._v("nn.Linear(in_features, out_features, bias=True)")]),s._v(" "),t("table",[t("thead",[t("tr",[t("th",[s._v("参数")]),s._v(" "),t("th",[s._v("含义")])])]),s._v(" "),t("tbody",[t("tr",[t("td",[t("code",[s._v("in_features")])]),s._v(" "),t("td",[s._v("输入特征的维度")])]),s._v(" "),t("tr",[t("td",[t("code",[s._v("out_features")])]),s._v(" "),t("td",[s._v("输出特征的维度")])]),s._v(" "),t("tr",[t("td",[t("code",[s._v("bias")])]),s._v(" "),t("td",[s._v("是否加偏置项（默认 "),t("code",[s._v("True")]),s._v("）")])])])]),s._v(" "),t("p",[s._v("nn.Linear 实现了一个 线性变换（矩阵乘+偏置），输入一个 shape 为 (batch_size, in_features) 的张量，输出 shape 为 (batch_size, out_features)，是深度学习中最常用的“全连接层”。")]),s._v(" "),t("p",[s._v("常用于：")]),s._v(" "),t("ul",[t("li",[s._v("输入 → 隐藏层映射")]),s._v(" "),t("li",[s._v("隐藏层 → 输出层映射")]),s._v(" "),t("li",[s._v("LSTM / CNN 的输出 → 分类层")])]),s._v(" "),t("h3",{attrs:{id:"nn-mseloss"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#nn-mseloss"}},[s._v("#")]),s._v(" nn.MSELoss")]),s._v(" "),t("p",[s._v("均方误差损失函数")]),s._v(" "),t("p",[t("img",{attrs:{src:"markdown-img/4%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.assets/MSELoss.png",alt:"image.png"}})]),s._v(" "),t("p",[s._v("常用")]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("loss = nn.MSELoss()\ninput = torch.randn(3, 5, requires_grad=True)\ntarget = torch.randn(3, 5)\noutput = loss(input, target)\noutput.backward()\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br")])]),t("h3",{attrs:{id:"nn-crossentropyloss"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#nn-crossentropyloss"}},[s._v("#")]),s._v(" nn.CrossEntropyLoss")]),s._v(" "),t("p",[s._v("交叉熵损失函数")])])}),[],!1,null,null,null);t.default=r.exports}}]);